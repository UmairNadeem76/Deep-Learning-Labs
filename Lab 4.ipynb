{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\nw = tf.Variable([.2], dtype=tf.float32)\nb = tf.Variable([-.2], dtype=tf.float32)\nx = tf.Variable([1, 2, 3, 4], dtype=tf.float32)  \ny = tf.Variable([0, -1, -2, -3], dtype=tf.float32)  \n\nLinear_Model = w * x + b\ninit = tf.compat.v1.global_variables_initializer()\n\ns=tf.compat.v1.Session()\ns.run(init)\nprint(\"Initial Weight:\",s.run(w))\nprint(\"Initial Bias:\",s.run(b))\n\nsquared_deltas = tf.square(Linear_Model - y)\nloss = tf.reduce_sum(squared_deltas)\n\nprint(\"Loss or Error is:\", s.run(loss))\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01) \ntrain = optimizer.minimize(loss)\ns.run(init)\nfor i in range(1000): \n    s.run(train)\n\nprint(\"Final Weight and Bias:\", s.run([w,b]))\ns.run(Linear_Model)\nprint(\"Loss Is:\", s.run(loss))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-09T18:28:02.717762Z","iopub.execute_input":"2024-05-09T18:28:02.718562Z","iopub.status.idle":"2024-05-09T18:28:03.179436Z","shell.execute_reply.started":"2024-05-09T18:28:02.718507Z","shell.execute_reply":"2024-05-09T18:28:03.178160Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Initial Weight: [0.2]\nInitial Bias: [-0.2]\nLoss or Error is: 20.16\nFinal Weight and Bias: [array([-0.649404], dtype=float32), array([0.02600318], dtype=float32)]\nLoss Is: 8.299139e-12\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# TASK NO:1","metadata":{}},{"cell_type":"markdown","source":"# Learning Rate = 0.1:\n\n**With such a high learning rate, the steps are too large, causing the optimizer to overshoot the minimum by a large margin. As a result, it can't converge and the values for weights and bias become \"not a number\" (`nan`), which means undefined.**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\nw = tf.Variable([.2], dtype=tf.float32)\nb = tf.Variable([-.2], dtype=tf.float32)\nx = tf.Variable([1, 2, 3, 4], dtype=tf.float32)  \ny = tf.Variable([0, -1, -2, -3], dtype=tf.float32)  \n\nLinear_Model = w * x + b\ninit = tf.compat.v1.global_variables_initializer()\n\ns=tf.compat.v1.Session()\ns.run(init)\nprint(\"Initial Weight:\",s.run(w))\nprint(\"Initial Bias:\",s.run(b))\n\nsquared_deltas = tf.square(Linear_Model - y)\nloss = tf.reduce_sum(squared_deltas)\n\nprint(\"Loss or Error is:\", s.run(loss))\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(0.1) \ntrain = optimizer.minimize(loss)\ns.run(init)\nfor i in range(1000): \n    s.run(train)\n\nprint(\"Final Weight and Bias:\", s.run([w,b]))\ns.run(Linear_Model)\nprint(\"Loss Is:\", s.run(loss))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:40:02.765183Z","iopub.execute_input":"2024-05-09T15:40:02.765563Z","iopub.status.idle":"2024-05-09T15:40:03.323940Z","shell.execute_reply.started":"2024-05-09T15:40:02.765534Z","shell.execute_reply":"2024-05-09T15:40:03.323102Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Initial Weight: [0.2]\nInitial Bias: [-0.2]\nLoss or Error is: 20.16\nFinal Weight and Bias: [array([nan], dtype=float32), array([nan], dtype=float32)]\nLoss Is: nan\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Learning Rate = 0.001:\n\n**The learning rate is relatively small. The optimizer takes smaller steps towards the minimum, preventing overshooting. It converges smoothly to a minimum loss and provides reasonable values for weights and bias.**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\nw = tf.Variable([.2], dtype=tf.float32)\nb = tf.Variable([-.2], dtype=tf.float32)\nx = tf.Variable([1, 2, 3, 4], dtype=tf.float32)  \ny = tf.Variable([0, -1, -2, -3], dtype=tf.float32)  \n\nLinear_Model = w * x + b\ninit = tf.compat.v1.global_variables_initializer()\n\ns=tf.compat.v1.Session()\ns.run(init)\nprint(\"Initial Weight:\",s.run(w))\nprint(\"Initial Bias:\",s.run(b))\n\nsquared_deltas = tf.square(Linear_Model - y)\nloss = tf.reduce_sum(squared_deltas)\n\nprint(\"Loss or Error is:\", s.run(loss))\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(0.001) \ntrain = optimizer.minimize(loss)\ns.run(init)\nfor i in range(1000): \n    s.run(train)\n\nprint(\"Final Weight and Bias:\", s.run([w,b]))\ns.run(Linear_Model)\nprint(\"Loss Is:\", s.run(loss))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:40:10.117223Z","iopub.execute_input":"2024-05-09T15:40:10.117601Z","iopub.status.idle":"2024-05-09T15:40:10.682331Z","shell.execute_reply.started":"2024-05-09T15:40:10.117570Z","shell.execute_reply":"2024-05-09T15:40:10.681291Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Initial Weight: [0.2]\nInitial Bias: [-0.2]\nLoss or Error is: 20.16\nFinal Weight and Bias: [array([-0.64520353], dtype=float32), array([0.02049667], dtype=float32)]\nLoss Is: 0.00032417607\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Learning Rate = 0.0001:\n\n**This learning rate is too small. The steps are too tiny, and the optimizer converges very slowly. It might get stuck in a shallow local minimum or take an exceedingly long time to reach the minimum.**\n","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\nw = tf.Variable([.2], dtype=tf.float32)\nb = tf.Variable([-.2], dtype=tf.float32)\nx = tf.Variable([1, 2, 3, 4], dtype=tf.float32)  \ny = tf.Variable([0, -1, -2, -3], dtype=tf.float32)  \n\nLinear_Model = w * x + b\ninit = tf.compat.v1.global_variables_initializer()\n\ns=tf.compat.v1.Session()\ns.run(init)\nprint(\"Initial Weight:\",s.run(w))\nprint(\"Initial Bias:\",s.run(b))\n\nsquared_deltas = tf.square(Linear_Model - y)\nloss = tf.reduce_sum(squared_deltas)\n\nprint(\"Loss or Error is:\", s.run(loss))\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(0.0001) \ntrain = optimizer.minimize(loss)\ns.run(init)\nfor i in range(1000): \n    s.run(train)\n\nprint(\"Final Weight and Bias:\", s.run([w,b]))\ns.run(Linear_Model)\nprint(\"Loss Is:\", s.run(loss))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:40:21.874575Z","iopub.execute_input":"2024-05-09T15:40:21.874967Z","iopub.status.idle":"2024-05-09T15:40:22.432267Z","shell.execute_reply.started":"2024-05-09T15:40:21.874936Z","shell.execute_reply":"2024-05-09T15:40:22.431178Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Initial Weight: [0.2]\nInitial Bias: [-0.2]\nLoss or Error is: 20.16\nFinal Weight and Bias: [array([-0.5385974], dtype=float32), array([-0.28686017], dtype=float32)]\nLoss Is: 0.66670716\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EXPLANATION:\n**Choosing the appropriate learning rate is crucial for model convergence \nand performance because it determines how fast or slow your model learns. The learning rate is \nlike the size of steps you take while trying to find the lowest point (minimum) of a hilly terrain. If \nyour steps are too large, you might overshoot the minimum and keep bouncing back and forth \nwithout getting closer to it. Conversely, if your steps are too small, you'll take a very long time to \nreach the minimum, or you might get stuck in a shallow local minimum instead of reaching the \nglobal minimum. Therefore, finding the right balance is essential for efficient training and \nachieving good performance.**","metadata":{}},{"cell_type":"markdown","source":"# TASK NO: 2","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.disable_eager_execution()\n\nw = tf.Variable([.2], dtype=tf.float32)\nb = tf.Variable([-.2], dtype=tf.float32)\nx = tf.Variable([ 1, 2, 2, 0, 0, 1], dtype=tf.float32)  \ny = tf.Variable([4, -4, -2, -2, 0, 0], dtype=tf.float32)  \n\nLinear_Model = w * x + b\ninit = tf.compat.v1.global_variables_initializer()\n\ns=tf.compat.v1.Session()\ns.run(init)\nprint(\"Initial Weight:\",s.run(w))\nprint(\"Initial Bias:\",s.run(b))\n\nsquared_deltas = tf.square(Linear_Model - y)\nloss = tf.reduce_sum(squared_deltas)\n\nprint(\"Loss or Error is:\", s.run(loss))\noptimizer = tf.compat.v1.train.GradientDescentOptimizer(0.01) \ntrain = optimizer.minimize(loss)\ns.run(init)\nfor i in range(1000): \n    s.run(train)\n\nprint(\"Final Weight and Bias:\", s.run([w,b]))\ns.run(Linear_Model)\nprint(\"Loss Is:\", s.run(loss))","metadata":{"execution":{"iopub.status.busy":"2024-05-09T15:39:19.817080Z","iopub.execute_input":"2024-05-09T15:39:19.817943Z","iopub.status.idle":"2024-05-09T15:39:20.336019Z","shell.execute_reply.started":"2024-05-09T15:39:19.817909Z","shell.execute_reply":"2024-05-09T15:39:20.335026Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Initial Weight: [0.2]\nInitial Bias: [-0.2]\nLoss or Error is: 41.760002\nFinal Weight and Bias: [array([-1.2820568], dtype=float32), array([0.37665004], dtype=float32)]\nLoss Is: 3.474554e-12\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EXPLANATION:\n**Adding more data points can improve the model's accuracy by providing a \nbroader range of examples for learning, reducing overfitting, and enhancing generalization. \nHowever, it may increase training time due to the larger dataset.**","metadata":{}}]}